{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6815de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "889e96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wbcd = pd.read_csv(\"Datasets\\wbcd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c055e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87139402</td>\n",
       "      <td>B</td>\n",
       "      <td>12.32</td>\n",
       "      <td>12.39</td>\n",
       "      <td>78.85</td>\n",
       "      <td>464.1</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.06981</td>\n",
       "      <td>0.03987</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>15.64</td>\n",
       "      <td>86.97</td>\n",
       "      <td>549.1</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>0.1266</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>0.09391</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.06771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8910251</td>\n",
       "      <td>B</td>\n",
       "      <td>10.60</td>\n",
       "      <td>18.95</td>\n",
       "      <td>69.28</td>\n",
       "      <td>346.4</td>\n",
       "      <td>0.09688</td>\n",
       "      <td>0.11470</td>\n",
       "      <td>0.06387</td>\n",
       "      <td>0.02642</td>\n",
       "      <td>...</td>\n",
       "      <td>11.88</td>\n",
       "      <td>22.94</td>\n",
       "      <td>78.28</td>\n",
       "      <td>424.8</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>0.2515</td>\n",
       "      <td>0.1916</td>\n",
       "      <td>0.07926</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.07587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>905520</td>\n",
       "      <td>B</td>\n",
       "      <td>11.04</td>\n",
       "      <td>16.83</td>\n",
       "      <td>70.92</td>\n",
       "      <td>373.2</td>\n",
       "      <td>0.10770</td>\n",
       "      <td>0.07804</td>\n",
       "      <td>0.03046</td>\n",
       "      <td>0.02480</td>\n",
       "      <td>...</td>\n",
       "      <td>12.41</td>\n",
       "      <td>26.44</td>\n",
       "      <td>79.93</td>\n",
       "      <td>471.4</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.1067</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2998</td>\n",
       "      <td>0.07881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0  87139402         B        12.32         12.39           78.85      464.1   \n",
       "1   8910251         B        10.60         18.95           69.28      346.4   \n",
       "2    905520         B        11.04         16.83           70.92      373.2   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  points_mean  ...  \\\n",
       "0          0.10280           0.06981         0.03987      0.03700  ...   \n",
       "1          0.09688           0.11470         0.06387      0.02642  ...   \n",
       "2          0.10770           0.07804         0.03046      0.02480  ...   \n",
       "\n",
       "   radius_worst  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0         13.50          15.64            86.97       549.1            0.1385   \n",
       "1         11.88          22.94            78.28       424.8            0.1213   \n",
       "2         12.41          26.44            79.93       471.4            0.1369   \n",
       "\n",
       "   compactness_worst  concavity_worst  points_worst  symmetry_worst  \\\n",
       "0             0.1266           0.1242       0.09391          0.2827   \n",
       "1             0.2515           0.1916       0.07926          0.2940   \n",
       "2             0.1482           0.1067       0.07431          0.2998   \n",
       "\n",
       "   dimension_worst  \n",
       "0          0.06771  \n",
       "1          0.07587  \n",
       "2          0.07881  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wbcd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b2de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "wbcd.drop([\"id\"],axis=1,inplace=True) # Dropping the uncessary column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83b620d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'points_mean', 'symmetry_mean', 'dimension_mean', 'radius_se',\n",
       "       'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'points_se', 'symmetry_se',\n",
       "       'dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst',\n",
       "       'area_worst', 'smoothness_worst', 'compactness_worst',\n",
       "       'concavity_worst', 'points_worst', 'symmetry_worst', 'dimension_worst'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wbcd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76c5186d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wbcd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6829037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis            0\n",
       "radius_mean          0\n",
       "texture_mean         0\n",
       "perimeter_mean       0\n",
       "area_mean            0\n",
       "smoothness_mean      0\n",
       "compactness_mean     0\n",
       "concavity_mean       0\n",
       "points_mean          0\n",
       "symmetry_mean        0\n",
       "dimension_mean       0\n",
       "radius_se            0\n",
       "texture_se           0\n",
       "perimeter_se         0\n",
       "area_se              0\n",
       "smoothness_se        0\n",
       "compactness_se       0\n",
       "concavity_se         0\n",
       "points_se            0\n",
       "symmetry_se          0\n",
       "dimension_se         0\n",
       "radius_worst         0\n",
       "texture_worst        0\n",
       "perimeter_worst      0\n",
       "area_worst           0\n",
       "smoothness_worst     0\n",
       "compactness_worst    0\n",
       "concavity_worst      0\n",
       "points_worst         0\n",
       "symmetry_worst       0\n",
       "dimension_worst      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wbcd.isnull().sum() # No missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6806dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wbcd.loc[wbcd.diagnosis==\"B\",\"diagnosis\"] = 1\n",
    "wbcd.loc[wbcd.diagnosis==\"M\",\"diagnosis\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef4fc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wbcd.drop([\"diagnosis\"],axis=1)\n",
    "Y = wbcd[\"diagnosis\"].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6f3830c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([212.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 357.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQsUlEQVR4nO3dbYxcV33H8e8PJwRaUJM0m8jYpnYj0+KgxqFbNyptFQhqQnjhRCKV0ypYKJKpmlQg8YKEFwVUWQoSD1XVBmQgwq0orlVC4/LUBheaIiBmg5wHx7i4JE0WW/HyVAiVUtn598XelKm96xnvzOyyJ9+PNJp7zz333v/RWr+5PnvnbqoKSVJbnrfUBUiSRs9wl6QGGe6S1CDDXZIaZLhLUoPOWuoCAC644IJau3btUpchScvK/fff/92qmphr289EuK9du5apqamlLkOSlpUk/znfNqdlJKlBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQT8T31CVpKW09tbPLNm5H7v99WM5bt8r9yQvSLIvyQNJDiR5d9f+riTfSbK/e13Ts89tSQ4nOZTkqrFULkma1yBX7k8Dr6mqp5KcDXw5yee6bR+oqvf2dk6yAdgCXAK8BPhCkpdV1YlRFi5Jml/fK/ea9VS3enb3Ot0fXt0M7Kqqp6vqUeAwsGnoSiVJAxvoF6pJViTZDxwD7qmq+7pNtyR5MMmdSc7r2lYBT/TsPt21nXzMbUmmkkzNzMwsfASSpFMMFO5VdaKqNgKrgU1JXgF8ELgY2AgcBd7Xdc9ch5jjmDuqarKqJicm5nwcsSRpgc7oVsiq+iHwJeDqqnqyC/1ngA/z06mXaWBNz26rgSPDlypJGtQgd8tMJDm3W34h8Frgm0lW9nS7Dni4W94DbElyTpJ1wHpg30irliSd1iB3y6wEdiZZweyHwe6q+nSSv0mykdkpl8eANwNU1YEku4FHgOPAzd4pI0mLq2+4V9WDwGVztN94mn22A9uHK02StFA+fkCSGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ3qG+5JXpBkX5IHkhxI8u6u/fwk9yT5Vvd+Xs8+tyU5nORQkqvGOQBJ0qkGuXJ/GnhNVV0KbASuTnI5cCuwt6rWA3u7dZJsALYAlwBXA3ckWTGG2iVJ8+gb7jXrqW717O5VwGZgZ9e+E7i2W94M7Kqqp6vqUeAwsGmURUuSTm+gOfckK5LsB44B91TVfcBFVXUUoHu/sOu+CniiZ/fpru3kY25LMpVkamZmZoghSJJONlC4V9WJqtoIrAY2JXnFabpnrkPMccwdVTVZVZMTExMDFStJGswZ3S1TVT8EvsTsXPqTSVYCdO/Hum7TwJqe3VYDR4YtVJI0uEHulplIcm63/ELgtcA3gT3A1q7bVuDubnkPsCXJOUnWAeuBfSOuW5J0GmcN0GclsLO74+V5wO6q+nSSrwK7k9wEPA5cD1BVB5LsBh4BjgM3V9WJ8ZQvSZpL33CvqgeBy+Zo/x5w5Tz7bAe2D12dJGlB/IaqJDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIa1Dfck6xJ8sUkB5McSPKWrv1dSb6TZH/3uqZnn9uSHE5yKMlV4xyAJOlUZw3Q5zjwtqr6RpIXA/cnuafb9oGqem9v5yQbgC3AJcBLgC8keVlVnRhl4ZKk+fW9cq+qo1X1jW75x8BBYNVpdtkM7Kqqp6vqUeAwsGkUxUqSBnNGc+5J1gKXAfd1TbckeTDJnUnO69pWAU/07DbNHB8GSbYlmUoyNTMzc+aVS5LmNXC4J3kR8EngrVX1I+CDwMXARuAo8L5nu86xe53SULWjqiaranJiYuJM65YkncZA4Z7kbGaD/eNVdRdAVT1ZVSeq6hngw/x06mUaWNOz+2rgyOhKliT1M8jdMgE+Chysqvf3tK/s6XYd8HC3vAfYkuScJOuA9cC+0ZUsSepnkLtlXgXcCDyUZH/X9g7ghiQbmZ1yeQx4M0BVHUiyG3iE2TttbvZOGUlaXH3Dvaq+zNzz6J89zT7bge1D1CVJGoLfUJWkBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNWiQv8T0M2/trZ9ZkvM+dvvrl+S8ktSPV+6S1CDDXZIaZLhLUoP6hnuSNUm+mORgkgNJ3tK1n5/kniTf6t7P69nntiSHkxxKctU4ByBJOtUgV+7HgbdV1cuBy4Gbk2wAbgX2VtV6YG+3TrdtC3AJcDVwR5IV4yhekjS3vuFeVUer6hvd8o+Bg8AqYDOws+u2E7i2W94M7Kqqp6vqUeAwsGnEdUuSTuOM5tyTrAUuA+4DLqqqozD7AQBc2HVbBTzRs9t013bysbYlmUoyNTMzs4DSJUnzGTjck7wI+CTw1qr60em6ztFWpzRU7aiqyaqanJiYGLQMSdIABgr3JGczG+wfr6q7uuYnk6zstq8EjnXt08Cant1XA0dGU64kaRCD3C0T4KPAwap6f8+mPcDWbnkrcHdP+5Yk5yRZB6wH9o2uZElSP4M8fuBVwI3AQ0n2d23vAG4Hdie5CXgcuB6gqg4k2Q08wuydNjdX1YlRFy5Jml/fcK+qLzP3PDrAlfPssx3YPkRdkqQh+A1VSWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1qG+4J7kzybEkD/e0vSvJd5Ls717X9Gy7LcnhJIeSXDWuwiVJ8xvkyv1jwNVztH+gqjZ2r88CJNkAbAEu6fa5I8mKURUrSRpM33CvqnuB7w94vM3Arqp6uqoeBQ4Dm4aoT5K0AMPMud+S5MFu2ua8rm0V8ERPn+mu7RRJtiWZSjI1MzMzRBmSpJMtNNw/CFwMbASOAu/r2jNH35rrAFW1o6omq2pyYmJigWVIkuayoHCvqier6kRVPQN8mJ9OvUwDa3q6rgaODFeiJOlMLSjck6zsWb0OePZOmj3AliTnJFkHrAf2DVeiJOlMndWvQ5JPAFcAFySZBt4JXJFkI7NTLo8BbwaoqgNJdgOPAMeBm6vqxFgqlyTNq2+4V9UNczR/9DT9twPbhylKkjQcv6EqSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QG9Q33JHcmOZbk4Z6285Pck+Rb3ft5PdtuS3I4yaEkV42rcEnS/Aa5cv8YcPVJbbcCe6tqPbC3WyfJBmALcEm3zx1JVoysWknSQPqGe1XdC3z/pObNwM5ueSdwbU/7rqp6uqoeBQ4Dm0ZTqiRpUAudc7+oqo4CdO8Xdu2rgCd6+k13badIsi3JVJKpmZmZBZYhSZrLqH+hmjnaaq6OVbWjqiaranJiYmLEZUjSc9tCw/3JJCsBuvdjXfs0sKan32rgyMLLkyQtxELDfQ+wtVveCtzd074lyTlJ1gHrgX3DlShJOlNn9euQ5BPAFcAFSaaBdwK3A7uT3AQ8DlwPUFUHkuwGHgGOAzdX1Ykx1S5JmkffcK+qG+bZdOU8/bcD24cpSpI0HL+hKkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBvX9A9mnk+Qx4MfACeB4VU0mOR/4O2At8Bjw+1X1g+HKlCSdiVFcub+6qjZW1WS3fiuwt6rWA3u7dUnSIhrHtMxmYGe3vBO4dgznkCSdxrDhXsA/J7k/ybau7aKqOgrQvV84145JtiWZSjI1MzMzZBmSpF5DzbkDr6qqI0kuBO5J8s1Bd6yqHcAOgMnJyRqyDklSj6Gu3KvqSPd+DPgUsAl4MslKgO792LBFSpLOzILDPcnPJ3nxs8vA7wEPA3uArV23rcDdwxYpSTozw0zLXAR8Ksmzx/nbqvp8kq8Du5PcBDwOXD98mZKkM7HgcK+qbwOXztH+PeDKYYqSJA3Hb6hKUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBYwv3JFcnOZTkcJJbx3UeSdKpxhLuSVYAfwW8DtgA3JBkwzjOJUk61biu3DcBh6vq21X1P8AuYPOYziVJOslZYzruKuCJnvVp4Dd7OyTZBmzrVp9KcmiI810AfHeI/Rck71nsM/6fJRnvEnPMzw3PuTHnPUON+Zfm2zCucM8cbfX/Vqp2ADtGcrJkqqomR3Gs5eC5Nl5wzM8Vjnl0xjUtMw2s6VlfDRwZ07kkSScZV7h/HVifZF2S5wNbgD1jOpck6SRjmZapquNJbgH+CVgB3FlVB8Zxrs5IpneWkefaeMExP1c45hFJVfXvJUlaVvyGqiQ1yHCXpAYtm3Dv9ziDzPqLbvuDSV65FHWO0gBj/sNurA8m+UqSS5eizlEa9LEVSX4jyYkkb1jM+sZhkDEnuSLJ/iQHkvzrYtc4agP82/6FJP+Y5IFuzG9aijpHJcmdSY4leXie7aPPr6r6mX8x+0vZ/wB+GXg+8ACw4aQ+1wCfY/Ye+8uB+5a67kUY828B53XLr3sujLmn378AnwXesNR1L8LP+VzgEeCl3fqFS133Ioz5HcB7uuUJ4PvA85e69iHG/LvAK4GH59k+8vxaLlfugzzOYDPw1zXra8C5SVYudqEj1HfMVfWVqvpBt/o1Zr9PsJwN+tiKPwE+CRxbzOLGZJAx/wFwV1U9DlBVy33cg4y5gBcnCfAiZsP9+OKWOTpVdS+zY5jPyPNruYT7XI8zWLWAPsvJmY7nJmY/+ZezvmNOsgq4DvjQItY1ToP8nF8GnJfkS0nuT/LGRatuPAYZ818CL2f2y48PAW+pqmcWp7wlMfL8GtfjB0at7+MMBuyznAw8niSvZjbcf3usFY3fIGP+c+DtVXVi9qJu2RtkzGcBvw5cCbwQ+GqSr1XVv4+7uDEZZMxXAfuB1wAXA/ck+beq+tGYa1sqI8+v5RLugzzOoLVHHgw0niS/BnwEeF1VfW+RahuXQcY8Cezqgv0C4Jokx6vqHxalwtEb9N/2d6vqJ8BPktwLXAos13AfZMxvAm6v2Qnpw0keBX4V2Lc4JS66kefXcpmWGeRxBnuAN3a/db4c+K+qOrrYhY5Q3zEneSlwF3DjMr6K69V3zFW1rqrWVtVa4O+BP17GwQ6D/du+G/idJGcl+Tlmn7B6cJHrHKVBxvw4s/9TIclFwK8A317UKhfXyPNrWVy51zyPM0jyR932DzF758Q1wGHgv5n95F+2BhzznwK/CNzRXcker2X8RL0Bx9yUQcZcVQeTfB54EHgG+EhVzXlL3XIw4M/5z4CPJXmI2SmLt1fVsn0UcJJPAFcAFySZBt4JnA3jyy8fPyBJDVou0zKSpDNguEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QG/S+cH6c631//PwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99b1fb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    357\n",
       "0    212\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wbcd.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8c46485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y) #Default percentage is 75 - 25 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "722e7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65fe40c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train) #computaion of Mean and Std to be used later as shown below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05262ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = scaler.fit_transform(X_train) #or else we can directly use fittransform to transform the data directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90d5a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR\n",
    "\n",
    "#def norm(i):\n",
    "#    temp=i-np.mean(i)/np.std(i)\n",
    "#    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22b377cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now apply standardization to the data\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2544c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43670b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a644b376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MLPClassifier in module sklearn.neural_network._multilayer_perceptron:\n",
      "\n",
      "class MLPClassifier(sklearn.base.ClassifierMixin, BaseMultilayerPerceptron)\n",
      " |  MLPClassifier(hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
      " |  \n",
      " |  Multi-layer Perceptron classifier.\n",
      " |  \n",
      " |  This model optimizes the log-loss function using LBFGS or stochastic\n",
      " |  gradient descent.\n",
      " |  \n",
      " |  .. versionadded:: 0.18\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  hidden_layer_sizes : tuple, length = n_layers - 2, default=(100,)\n",
      " |      The ith element represents the number of neurons in the ith\n",
      " |      hidden layer.\n",
      " |  \n",
      " |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\n",
      " |      Activation function for the hidden layer.\n",
      " |  \n",
      " |      - 'identity', no-op activation, useful to implement linear bottleneck,\n",
      " |        returns f(x) = x\n",
      " |  \n",
      " |      - 'logistic', the logistic sigmoid function,\n",
      " |        returns f(x) = 1 / (1 + exp(-x)).\n",
      " |  \n",
      " |      - 'tanh', the hyperbolic tan function,\n",
      " |        returns f(x) = tanh(x).\n",
      " |  \n",
      " |      - 'relu', the rectified linear unit function,\n",
      " |        returns f(x) = max(0, x)\n",
      " |  \n",
      " |  solver : {'lbfgs', 'sgd', 'adam'}, default='adam'\n",
      " |      The solver for weight optimization.\n",
      " |  \n",
      " |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
      " |  \n",
      " |      - 'sgd' refers to stochastic gradient descent.\n",
      " |  \n",
      " |      - 'adam' refers to a stochastic gradient-based optimizer proposed\n",
      " |        by Kingma, Diederik, and Jimmy Ba\n",
      " |  \n",
      " |      Note: The default solver 'adam' works pretty well on relatively\n",
      " |      large datasets (with thousands of training samples or more) in terms of\n",
      " |      both training time and validation score.\n",
      " |      For small datasets, however, 'lbfgs' can converge faster and perform\n",
      " |      better.\n",
      " |  \n",
      " |  alpha : float, default=0.0001\n",
      " |      L2 penalty (regularization term) parameter.\n",
      " |  \n",
      " |  batch_size : int, default='auto'\n",
      " |      Size of minibatches for stochastic optimizers.\n",
      " |      If the solver is 'lbfgs', the classifier will not use minibatch.\n",
      " |      When set to \"auto\", `batch_size=min(200, n_samples)`\n",
      " |  \n",
      " |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'\n",
      " |      Learning rate schedule for weight updates.\n",
      " |  \n",
      " |      - 'constant' is a constant learning rate given by\n",
      " |        'learning_rate_init'.\n",
      " |  \n",
      " |      - 'invscaling' gradually decreases the learning rate at each\n",
      " |        time step 't' using an inverse scaling exponent of 'power_t'.\n",
      " |        effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
      " |  \n",
      " |      - 'adaptive' keeps the learning rate constant to\n",
      " |        'learning_rate_init' as long as training loss keeps decreasing.\n",
      " |        Each time two consecutive epochs fail to decrease training loss by at\n",
      " |        least tol, or fail to increase validation score by at least tol if\n",
      " |        'early_stopping' is on, the current learning rate is divided by 5.\n",
      " |  \n",
      " |      Only used when ``solver='sgd'``.\n",
      " |  \n",
      " |  learning_rate_init : double, default=0.001\n",
      " |      The initial learning rate used. It controls the step-size\n",
      " |      in updating the weights. Only used when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  power_t : double, default=0.5\n",
      " |      The exponent for inverse scaling learning rate.\n",
      " |      It is used in updating effective learning rate when the learning_rate\n",
      " |      is set to 'invscaling'. Only used when solver='sgd'.\n",
      " |  \n",
      " |  max_iter : int, default=200\n",
      " |      Maximum number of iterations. The solver iterates until convergence\n",
      " |      (determined by 'tol') or this number of iterations. For stochastic\n",
      " |      solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
      " |      (how many times each data point will be used), not the number of\n",
      " |      gradient steps.\n",
      " |  \n",
      " |  shuffle : bool, default=True\n",
      " |      Whether to shuffle samples in each iteration. Only used when\n",
      " |      solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Determines random number generation for weights and bias\n",
      " |      initialization, train-test split if early stopping is used, and batch\n",
      " |      sampling when solver='sgd' or 'adam'.\n",
      " |      Pass an int for reproducible results across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for the optimization. When the loss or score is not improving\n",
      " |      by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n",
      " |      unless ``learning_rate`` is set to 'adaptive', convergence is\n",
      " |      considered to be reached and training stops.\n",
      " |  \n",
      " |  verbose : bool, default=False\n",
      " |      Whether to print progress messages to stdout.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous\n",
      " |      call to fit as initialization, otherwise, just erase the\n",
      " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  momentum : float, default=0.9\n",
      " |      Momentum for gradient descent update. Should be between 0 and 1. Only\n",
      " |      used when solver='sgd'.\n",
      " |  \n",
      " |  nesterovs_momentum : bool, default=True\n",
      " |      Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
      " |      momentum > 0.\n",
      " |  \n",
      " |  early_stopping : bool, default=False\n",
      " |      Whether to use early stopping to terminate training when validation\n",
      " |      score is not improving. If set to true, it will automatically set\n",
      " |      aside 10% of training data as validation and terminate training when\n",
      " |      validation score is not improving by at least tol for\n",
      " |      ``n_iter_no_change`` consecutive epochs. The split is stratified,\n",
      " |      except in a multilabel setting.\n",
      " |      If early stopping is False, then the training stops when the training\n",
      " |      loss does not improve by more than tol for n_iter_no_change consecutive\n",
      " |      passes over the training set.\n",
      " |      Only effective when solver='sgd' or 'adam'\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if early_stopping is True\n",
      " |  \n",
      " |  beta_1 : float, default=0.9\n",
      " |      Exponential decay rate for estimates of first moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'\n",
      " |  \n",
      " |  beta_2 : float, default=0.999\n",
      " |      Exponential decay rate for estimates of second moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'\n",
      " |  \n",
      " |  epsilon : float, default=1e-8\n",
      " |      Value for numerical stability in adam. Only used when solver='adam'\n",
      " |  \n",
      " |  n_iter_no_change : int, default=10\n",
      " |      Maximum number of epochs to not meet ``tol`` improvement.\n",
      " |      Only effective when solver='sgd' or 'adam'\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  max_fun : int, default=15000\n",
      " |      Only used when solver='lbfgs'. Maximum number of loss function calls.\n",
      " |      The solver iterates until convergence (determined by 'tol'), number\n",
      " |      of iterations reaches max_iter, or this number of loss function calls.\n",
      " |      Note that number of loss function calls will be greater than or equal\n",
      " |      to the number of iterations for the `MLPClassifier`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray or list of ndarray of shape (n_classes,)\n",
      " |      Class labels for each output.\n",
      " |  \n",
      " |  loss_ : float\n",
      " |      The current loss computed with the loss function.\n",
      " |  \n",
      " |  best_loss_ : float\n",
      " |      The minimum loss reached by the solver throughout fitting.\n",
      " |  \n",
      " |  loss_curve_ : list of shape (`n_iter_`,)\n",
      " |      The ith element in the list represents the loss at the ith iteration.\n",
      " |  \n",
      " |  t_ : int\n",
      " |      The number of training samples seen by the solver during fitting.\n",
      " |  \n",
      " |  coefs_ : list of shape (n_layers - 1,)\n",
      " |      The ith element in the list represents the weight matrix corresponding\n",
      " |      to layer i.\n",
      " |  \n",
      " |  intercepts_ : list of shape (n_layers - 1,)\n",
      " |      The ith element in the list represents the bias vector corresponding to\n",
      " |      layer i + 1.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      The number of iterations the solver has run.\n",
      " |  \n",
      " |  n_layers_ : int\n",
      " |      Number of layers.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      Number of outputs.\n",
      " |  \n",
      " |  out_activation_ : str\n",
      " |      Name of the output activation function.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.neural_network import MLPClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> from sklearn.model_selection import train_test_split\n",
      " |  >>> X, y = make_classification(n_samples=100, random_state=1)\n",
      " |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
      " |  ...                                                     random_state=1)\n",
      " |  >>> clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
      " |  >>> clf.predict_proba(X_test[:1])\n",
      " |  array([[0.038..., 0.961...]])\n",
      " |  >>> clf.predict(X_test[:5, :])\n",
      " |  array([1, 0, 1, 0, 1])\n",
      " |  >>> clf.score(X_test, y_test)\n",
      " |  0.8...\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  MLPClassifier trains iteratively since at each time step\n",
      " |  the partial derivatives of the loss function with respect to the model\n",
      " |  parameters are computed to update the parameters.\n",
      " |  \n",
      " |  It can also have a regularization term added to the loss function\n",
      " |  that shrinks model parameters to prevent overfitting.\n",
      " |  \n",
      " |  This implementation works with data represented as dense numpy arrays or\n",
      " |  sparse scipy arrays of floating point values.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  Hinton, Geoffrey E.\n",
      " |      \"Connectionist learning procedures.\" Artificial intelligence 40.1\n",
      " |      (1989): 185-234.\n",
      " |  \n",
      " |  Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n",
      " |      training deep feedforward neural networks.\" International Conference\n",
      " |      on Artificial Intelligence and Statistics. 2010.\n",
      " |  \n",
      " |  He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n",
      " |      performance on imagenet classification.\" arXiv preprint\n",
      " |      arXiv:1502.01852 (2015).\n",
      " |  \n",
      " |  Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n",
      " |      optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MLPClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseMultilayerPerceptron\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the multi-layer perceptron classifier\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray, shape (n_samples,) or (n_samples, n_classes)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return the log of probability estimates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      log_y_prob : ndarray of shape (n_samples, n_classes)\n",
      " |          The predicted log-probability of the sample for each class\n",
      " |          in the model, where classes are ordered as they are in\n",
      " |          `self.classes_`. Equivalent to log(predict_proba(X))\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_prob : ndarray of shape (n_samples, n_classes)\n",
      " |          The predicted probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in `self.classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  partial_fit\n",
      " |      Update the model with a single iteration over the given data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target values.\n",
      " |      \n",
      " |      classes : array of shape (n_classes,), default=None\n",
      " |          Classes across all calls to partial_fit.\n",
      " |          Can be obtained via `np.unique(y_all)`, where y_all is the\n",
      " |          target vector of the entire dataset.\n",
      " |          This argument is required for the first call to partial_fit\n",
      " |          and can be omitted in the subsequent calls.\n",
      " |          Note that y doesn't need to contain all labels in `classes`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseMultilayerPerceptron:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model to data matrix X and target(s) y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray or sparse matrix of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MLPClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8fc43ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pranav\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(30, 30))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9595dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67a71868",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_train = mlp.predict(X_train)\n",
    "prediction_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "78519dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[162   0]\n",
      " [  0 264]]\n",
      "\n",
      "\n",
      "[[49  1]\n",
      " [ 1 92]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_train,prediction_train))\n",
    "print(\"\\n\")\n",
    "print(confusion_matrix(Y_test,prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6101532f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 30)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "66a1ec6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diagnosis</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0        0    1\n",
       "diagnosis          \n",
       "0          162    0\n",
       "1            0  264"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(Y_train,prediction_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8847935c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diagnosis</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0       0   1\n",
       "diagnosis        \n",
       "0          49   1\n",
       "1           1  92"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(Y_test,prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9359f5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
